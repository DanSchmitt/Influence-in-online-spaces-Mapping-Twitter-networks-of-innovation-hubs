{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import operator \n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import operator \n",
    "import json\n",
    "\n",
    "\n",
    "from nltk import bigrams \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import time\n",
    "import calendar\n",
    "import codecs\n",
    "import datetime\n",
    "import sys\n",
    "import gzip\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READING & COUNTING TWEETS\n",
    "\n",
    "frequencyMap = {}\n",
    "globalTweetCounter = 0\n",
    "\n",
    "timeFormat = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "reader = codecs.getreader(\"utf-8\")\n",
    "\n",
    "with open('combined_data.json', 'r') as f:\n",
    "    tweet = json.load(f)\n",
    "    for item in tweet:\n",
    "        if ( \"delete\" in item.keys() or \"status_withheld\" in item.keys() ):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            currentTime = datetime.datetime.strptime(item['created_at'], timeFormat)\n",
    "        except:\n",
    "            print (item)\n",
    "            raise\n",
    "\n",
    "        currentTime = currentTime.replace(second=0)\n",
    "            \n",
    "        globalTweetCounter += 1\n",
    "            \n",
    "        if ( currentTime in frequencyMap.keys() ):\n",
    "            timeMap = frequencyMap[currentTime]\n",
    "            timeMap[\"count\"] += 1\n",
    "            timeMap[\"list\"].append(item)\n",
    "        else:\n",
    "            frequencyMap[currentTime] = {\"count\":1, \"list\":[item]}\n",
    "                \n",
    "times = sorted(frequencyMap.keys())\n",
    "firstTime = times[0]\n",
    "lastTime = times[-1]\n",
    "thisTime = firstTime\n",
    "\n",
    "timeIntervalStep = datetime.timedelta(0, 60)    # Time step in seconds\n",
    "while ( thisTime <= lastTime ):\n",
    "    if ( thisTime not in frequencyMap.keys() ):\n",
    "        frequencyMap[thisTime] = {\"count\":0, \"list\":[]}\n",
    "        \n",
    "    thisTime = thisTime + timeIntervalStep\n",
    "\n",
    "print (\"Total Tweet count:\", globalTweetCounter)\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TWITTER TIMELINE\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5,10.5)\n",
    "\n",
    "plt.title(\"Tweet Time Series\")\n",
    "\n",
    "sortedTimes = sorted(frequencyMap.keys())\n",
    "\n",
    "postFreqList = [frequencyMap[x][\"count\"] for x in sortedTimes]\n",
    "\n",
    "smallerXTicks = range(0, len(sortedTimes), 43200)\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "ax.plot(range(len(frequencyMap)), [x if x > 0 else 0 for x in postFreqList], color=\"black\", label=\"Tweets\")\n",
    "ax.grid(b=True, which=u'major')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMBER OF UNIQUE USERS\n",
    "\n",
    "globalUserCounter = {}\n",
    "globalUserMap = {}\n",
    "\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        user = tweet[\"user\"][\"screen_name\"]\n",
    "        \n",
    "        if ( user not in globalUserCounter ):\n",
    "            globalUserCounter[user] = 1\n",
    "            globalUserMap[user] = [tweet]\n",
    "        else:\n",
    "            globalUserCounter[user] += 1\n",
    "            globalUserMap[user].append(tweet)\n",
    "\n",
    "print (\"Unique Users: \" + str(len(globalUserCounter.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OP 10 MOST ACTIVE USERS\n",
    "\n",
    "sortedUsers = sorted(globalUserCounter, key=globalUserCounter.get, reverse=True)\n",
    "print (\"Top most active users:\")\n",
    "for u in sortedUsers[:10]:\n",
    "    print (u, globalUserCounter[u])\n",
    "\n",
    "#GETTING INFORMATION ON MOST ACTIVE USERS\n",
    "\n",
    "import tweepy\n",
    "\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.secure = True\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "print (\"User + description:\")\n",
    "for u in sortedUsers[:50]:\n",
    "    print (u, globalUserCounter[u])\n",
    "\n",
    "    try:\n",
    "        user = api.get_user(u)\n",
    "        print (\"\\tDescription:\", user.description)\n",
    "    except Exception as te:\n",
    "        print (\"\\tDescription Error:\", te)\n",
    "        \n",
    "    print (\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HISTOGRAM --> NUMBER OF ALL USERS TWEETS\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.hist(\n",
    "    [globalUserCounter[x] for x in globalUserCounter], \n",
    "    bins=100, \n",
    "    normed=0, \n",
    "    alpha=1,\n",
    "    label=\"Tweet Count\",\n",
    "    log=True,\n",
    "    color='black')\n",
    "\n",
    "plt.xlabel('Tweets')\n",
    "plt.ylabel('Users')\n",
    "plt.title(\"Histogram of Unique User Tweets\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVERAGE NUMBER OF POSTS\n",
    "\n",
    "avgPostCount = np.mean([globalUserCounter[x] for x in globalUserCounter])\n",
    "print(\"Average Number of Posts: \" + str(avgPostCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOST POPULAR HASHTAGS\n",
    "\n",
    "hashtagCounter = {}\n",
    "\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        hashtagList = tweet[\"entities\"][\"hashtags\"]\n",
    "        \n",
    "        for hashtagObj in hashtagList:\n",
    "            \n",
    "            hashtagString = hashtagObj[\"text\"].lower()\n",
    "            \n",
    "            if ( hashtagString not in hashtagCounter ):\n",
    "                hashtagCounter[hashtagString] = 1\n",
    "            else:\n",
    "                hashtagCounter[hashtagString] += 1\n",
    "\n",
    "print (\"Unique Hashtags:\", len(hashtagCounter.keys()))\n",
    "sortedHashtags = sorted(hashtagCounter, key=hashtagCounter.get, reverse=True)\n",
    "print (\"Top Fifty Hashtags:\")\n",
    "for ht in sortedHashtags[:50]:\n",
    "    print (\"\\t\", \"#\" + ht, hashtagCounter[ht])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LANGUAGE DISTRIBUTION LIST\n",
    "\n",
    "languageCounter = {}\n",
    "\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "    \n",
    "    for tweet in timeObj[\"list\"]:\n",
    "        lang = tweet[\"lang\"]\n",
    "        \n",
    "        if ( lang not in languageCounter ):\n",
    "            languageCounter[lang] = 1\n",
    "        else:\n",
    "            languageCounter[lang] += 1\n",
    "            \n",
    "languages = sorted(languageCounter.keys(), key=languageCounter.get, reverse=True)\n",
    "\n",
    "for l in languages:\n",
    "    print (l, languageCounter[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "  \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',\n",
    "    r'(?:@[\\w_]+)',\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', \n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",\n",
    "    r'(?:[\\w_]+)', \n",
    "    r'(?:\\S)'\n",
    "    \n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "with open('combined_data.json', 'r') as f:\n",
    "    tweets_all = []\n",
    "    tweet = json.load(f)\n",
    "    for item in tweet:\n",
    "        \n",
    "        # <140 : ['text']\n",
    "        # >140 : ['extended_tweet']['full_text']\n",
    "        # >140 & retweet (RT) : ['retweeted_status]['extended_tweet']['full_text']\n",
    "        \n",
    "        if item.get('retweeted_status'):\n",
    "            if item['retweeted_status'].get('extended_tweet'):\n",
    "                tokens_all = preprocess(item['retweeted_status']['extended_tweet']['full_text'].lower())\n",
    "            else:\n",
    "                tokens_all = preprocess(item['retweeted_status']['text'].lower())\n",
    "        else:\n",
    "            if item.get('extended_tweet'):\n",
    "                tokens_all = preprocess(item['extended_tweet']['full_text'].lower())\n",
    "            else:\n",
    "                tokens_all = preprocess(item['text'].lower()) \n",
    "            \n",
    "        tweets_all.append(tokens_all)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords & twitter specific terms\n",
    "punctuation = list(string.punctuation)\n",
    "stop = list(stopwords.words('english') + stopwords.words('spanish') + stopwords.words('french') + punctuation + ['RT', 'via', 'amp', 'The', 'I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting most frequent terms & n-grams\n",
    "\n",
    "count_all = Counter()\n",
    "tweets_filtered = []\n",
    "for term in tweets_all:\n",
    "    if term not in stop:\n",
    "        tweets_filtered += term\n",
    "count_all.update(tweets_filtered)  \n",
    "print(\"most frequent terms:\", count_all.most_common(100))\n",
    "print('________________________')\n",
    "    \n",
    "terms_bigram = bigrams(tweets_filtered) #exclude stopwords\n",
    "count_all_bg = Counter()\n",
    "count_all_bg.update(terms_bigram)\n",
    "print(\"most frequent bigrams:\", count_all_bg.most_common(300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TERM CO-OCCURRENCE\n",
    "\n",
    "from collections import defaultdict\n",
    " \n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    "        \n",
    "with open('combined_data.json', 'r') as f:\n",
    "    tweets_all = []\n",
    "    tweet = json.load(f)\n",
    "    for item in tweet:\n",
    "        if item.get('retweeted_status'):\n",
    "            if item['retweeted_status'].get('extended_tweet'):\n",
    "                tokens_all = preprocess(item['retweeted_status']['extended_tweet']['full_text'].lower())\n",
    "            else:\n",
    "                tokens_all = preprocess(item['retweeted_status']['text'].lower())\n",
    "        else:\n",
    "            if item.get('extended_tweet'):\n",
    "                tokens_all = preprocess(item['extended_tweet']['full_text'].lower())\n",
    "            else:\n",
    "                tokens_all = preprocess(item['text'].lower())\n",
    "        \n",
    "        terms_only = [term for term in tokens_all if term not in stop]\n",
    "        \n",
    "        for i in range(len(terms_only)-1):            \n",
    "            for j in range(i+1, len(terms_only)):\n",
    "                w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "                if w1 != w2:\n",
    "                    com[w1][w2] += 1\n",
    "\n",
    "                \n",
    "com_max = []\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[50:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#co-occurence with specific word\n",
    "with open('combined_data.json', 'r') as f:\n",
    "    search_word = 'innovation'\n",
    "    count_search = Counter()\n",
    "    tweets_all = []\n",
    "    tweet = json.load(f)\n",
    "    for item in tweet:\n",
    "        if item.get('retweeted_status'):\n",
    "            if item['retweeted_status'].get('extended_tweet'):\n",
    "                tokens_all = preprocess(item['retweeted_status']['extended_tweet']['full_text'].lower())\n",
    "            else:\n",
    "                tokens_all = preprocess(item['retweeted_status']['text'].lower())\n",
    "        else:\n",
    "            if item.get('extended_tweet'):\n",
    "                tokens_all = preprocess(item['extended_tweet']['full_text'].lower())\n",
    "            else:\n",
    "                tokens_all = preprocess(item['text'].lower())\n",
    "        \n",
    "        terms_only = [term for term in tokens_all if term not in stop]\n",
    "        if search_word in terms_only:\n",
    "            count_search.update(terms_only)\n",
    "    print(\"Co-occurrence for %s:\" % search_word)\n",
    "    print(count_search.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#keywords\n",
    "targetKeywords = [\"python\"]\n",
    "\n",
    "targetCounts = {x:[] for x in targetKeywords}\n",
    "totalCount = []\n",
    "\n",
    "for t in sortedTimes:\n",
    "    timeObj = frequencyMap[t]\n",
    "  \n",
    "    localTargetCounts = {x:0 for x in targetKeywords}\n",
    "    localTotalCount = 0\n",
    "    \n",
    "    for tweetObj in timeObj[\"list\"]:\n",
    "        if tweetObj.get('retweeted_status'):\n",
    "            if tweetObj['retweeted_status'].get('extended_tweet'):\n",
    "                    tweetString = tweetObj['retweeted_status']['extended_tweet']['full_text'].lower()\n",
    "            else:\n",
    "                    tweetString = tweetObj['retweeted_status']['text'].lower()\n",
    "        else:\n",
    "            if tweetObj.get('extended_tweet'):\n",
    "                tweetString = tweetObj['extended_tweet']['full_text'].lower()\n",
    "            else:\n",
    "                tweetString = tweetObj['text'].lower()\n",
    "\n",
    "        localTotalCount += 1\n",
    "        \n",
    "        for keyword in targetKeywords:\n",
    "            if ( keyword in tweetString ):\n",
    "                localTargetCounts[keyword] += 1\n",
    " \n",
    "    totalCount.append(localTotalCount)\n",
    "    for keyword in targetKeywords:\n",
    "        targetCounts[keyword].append(localTargetCounts[keyword])\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5,10.5)\n",
    "\n",
    "plt.title(\"Time series with term 'Python''\")\n",
    "plt.xticks(smallerXTicks, [sortedTimes[x] for x in smallerXTicks], rotation=90)\n",
    "\n",
    "ax.plot(range(len(frequencyMap)), totalCount, label=\"Total\", color='black')\n",
    "\n",
    "for keyword in targetKeywords:\n",
    "    ax.plot(range(len(frequencyMap)), targetCounts[keyword], label=keyword, color='grey')\n",
    "ax.legend()\n",
    "ax.grid(b=True, which=u'major')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUMBER OF TWEETS WITH GEO DATA\n",
    "\n",
    "geoFrequencyMap = {}\n",
    "geoCount = 0\n",
    "\n",
    "for t in sortedTimes:\n",
    "    geos = list(filter(lambda tweet: tweet[\"coordinates\"] != None and \"coordinates\" in tweet[\"coordinates\"], frequencyMap[t][\"list\"]))\n",
    "    geoCount += len(geos)\n",
    "    \n",
    "    geoFrequencyMap[t] = {\"count\": len(geos), \"list\": geos}\n",
    "\n",
    "print (\"Number of tweets with geodata: \" + str(geoCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'combined_data.json'\n",
    "with open(fname, 'r') as f:\n",
    " \n",
    "    users_with_geodata = {\n",
    "        \"data\": []\n",
    "    }\n",
    "    all_users = []\n",
    "    total_tweets = 0\n",
    "    geo_tweets  = 0\n",
    "    line = json.load(f)\n",
    "    for tweet in line:\n",
    "        if tweet['user']['id']:\n",
    "            total_tweets += 1 \n",
    "            user_id = tweet['user']['id']\n",
    "            if user_id not in all_users:\n",
    "                all_users.append(user_id)\n",
    "              \n",
    "                user_data = {\n",
    "                    \"user_id\" : tweet['user']['id'],\n",
    "                    \"features\" : {\n",
    "                        \"name\" : tweet['user']['name'],\n",
    "                        \"id\": tweet['user']['id'],\n",
    "                        \"screen_name\": tweet['user']['screen_name'],\n",
    "                        \"tweets\" : 1,\n",
    "                        \"location\": tweet['user']['location'],\n",
    "                    }\n",
    "                }\n",
    "            \n",
    "                if tweet['coordinates']:\n",
    "                    user_data[\"features\"][\"primary_geo\"] = str(tweet['coordinates'][tweet['coordinates'].keys()[1]][1]) + \", \" + str(tweet['coordinates'][tweet['coordinates'].keys()[1]][0])\n",
    "                    user_data[\"features\"][\"geo_type\"] = \"Tweet coordinates\"\n",
    "                elif tweet['place']:\n",
    "                    user_data[\"features\"][\"primary_geo\"] = tweet['place']['full_name'] + \", \" + tweet['place']['country']\n",
    "                    user_data[\"features\"][\"geo_type\"] = \"Tweet place\"\n",
    "                else:\n",
    "                    user_data[\"features\"][\"primary_geo\"] = tweet['user']['location']\n",
    "                    user_data[\"features\"][\"geo_type\"] = \"User location\"\n",
    "            \n",
    "                if user_data[\"features\"][\"primary_geo\"]:\n",
    "                    users_with_geodata['data'].append(user_data)\n",
    "                    geo_tweets += 1\n",
    "            \n",
    "            elif user_id in all_users:\n",
    "                for user in users_with_geodata[\"data\"]:\n",
    "                    if user_id == user[\"user_id\"]:\n",
    "                        user[\"features\"][\"tweets\"] += 1\n",
    "               \n",
    "    for user in users_with_geodata[\"data\"]:\n",
    "        geo_tweets = geo_tweets + user[\"features\"][\"tweets\"]\n",
    "    \n",
    "    print \"The file included \" + str(len(users_with_geodata['data'])) + \" unique users who tweeted with geo data, including 'location'\"\n",
    "\n",
    "with open('geo_data.json', 'w') as fout:\n",
    "    fout.write(json.dumps(users_with_geodata, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
